{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP #2\n",
    "Baseline Neural Network Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.read_csv(\"sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "x = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "encoder = LabelEncoder()\n",
    "A = encoder.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    model = tensorflow.keras.Sequential()\n",
    "    model.add(layers.Dense(60 , activation='relu' , input_shape=(60 ,)))\n",
    "    model.add(layers.Dense(1 , activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='Adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 82.23% (6.72%)\n"
     ]
    }
   ],
   "source": [
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X, A, cv=kfold)\n",
    "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3\n",
    "Re-Run The Baseline Model With Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    model = tensorflow.keras.Sequential()\n",
    "    model.add(layers.Dense(60 , activation='relu' , input_shape=(60 ,)))\n",
    "    model.add(layers.Dense(1 , activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='Adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 84.09% (5.74%)\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, A, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP #4\n",
    "Tuning Layers and Number of Neurons in The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4.1\n",
    "Evaluate a Smaller Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_smaller():\n",
    "    \n",
    "    model = tensorflow.keras.Sequential()\n",
    "    model.add(layers.Dense(30 , activation='relu' , input_shape=(60 ,)))\n",
    "    model.add(layers.Dense(1 , activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='Adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 85.09% (5.01%)\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, A, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2\n",
    "Evaluate a Larger Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_larger():\n",
    "    \n",
    "    model = tensorflow.keras.Sequential()\n",
    "    model.add(layers.Dense(60 , activation='relu' , input_shape=(60 ,)))\n",
    "    model.add(layers.Dense(30 , activation='relu' ))\n",
    "    model.add(layers.Dense(1 , activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='Adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 84.09% (6.12%)\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, A, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP #5\n",
    "Really Scaling up: developing a model that overfits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    model = tensorflow.keras.Sequential()\n",
    "    model.add(layers.Dense(60 , activation='relu' , input_shape=(60 ,)))\n",
    "    model.add(layers.Dense(30 , activation='relu' ))\n",
    "    model.add(layers.Dense(30 , activation='relu' ))\n",
    "    model.add(layers.Dense(16 , activation='relu' ))\n",
    "    model.add(layers.Dense(1 , activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='Adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 85.04% (5.62%)\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, A, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP #6\n",
    "Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_baseline():\n",
    "    model = tensorflow.keras.Sequential()\n",
    "    model.add(layers.Dense( 120, activation='relu' , input_shape=(60 ,)))\n",
    "    model.add(layers.Dense(1 , activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='Adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 83.64% (5.75%)\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, A, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP #7\n",
    "Rewriting the code using the Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs= tensorflow.keras.Input(shape=(60,))\n",
    "X = layers.Dense(60 , activation='relu')(inputs)\n",
    "outputs = layers.Dense(1 , activation='sigmoid')(X)\n",
    "    \n",
    "model = tensorflow.keras.Model(inputs , outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 208 samples\n",
      "Epoch 1/100\n",
      "208/208 [==============================] - 2s 11ms/sample - loss: 0.7099 - accuracy: 0.4904\n",
      "Epoch 2/100\n",
      "208/208 [==============================] - 0s 923us/sample - loss: 0.6687 - accuracy: 0.5913\n",
      "Epoch 3/100\n",
      "208/208 [==============================] - 0s 917us/sample - loss: 0.6407 - accuracy: 0.6731\n",
      "Epoch 4/100\n",
      "208/208 [==============================] - 0s 817us/sample - loss: 0.6135 - accuracy: 0.7115\n",
      "Epoch 5/100\n",
      "208/208 [==============================] - 0s 846us/sample - loss: 0.5961 - accuracy: 0.7163\n",
      "Epoch 6/100\n",
      "208/208 [==============================] - 0s 711us/sample - loss: 0.5718 - accuracy: 0.7212\n",
      "Epoch 7/100\n",
      "208/208 [==============================] - 0s 676us/sample - loss: 0.5501 - accuracy: 0.7644\n",
      "Epoch 8/100\n",
      "208/208 [==============================] - 0s 726us/sample - loss: 0.5312 - accuracy: 0.7644\n",
      "Epoch 9/100\n",
      "208/208 [==============================] - 0s 692us/sample - loss: 0.5108 - accuracy: 0.7837\n",
      "Epoch 10/100\n",
      "208/208 [==============================] - 0s 560us/sample - loss: 0.4995 - accuracy: 0.7644\n",
      "Epoch 11/100\n",
      "208/208 [==============================] - 0s 489us/sample - loss: 0.4872 - accuracy: 0.7933\n",
      "Epoch 12/100\n",
      "208/208 [==============================] - 0s 486us/sample - loss: 0.4735 - accuracy: 0.7837\n",
      "Epoch 13/100\n",
      "208/208 [==============================] - 0s 519us/sample - loss: 0.4673 - accuracy: 0.7981\n",
      "Epoch 14/100\n",
      "208/208 [==============================] - 0s 491us/sample - loss: 0.4451 - accuracy: 0.7933\n",
      "Epoch 15/100\n",
      "208/208 [==============================] - 0s 538us/sample - loss: 0.4425 - accuracy: 0.8125\n",
      "Epoch 16/100\n",
      "208/208 [==============================] - 0s 521us/sample - loss: 0.4287 - accuracy: 0.8125\n",
      "Epoch 17/100\n",
      "208/208 [==============================] - 0s 544us/sample - loss: 0.4242 - accuracy: 0.8125\n",
      "Epoch 18/100\n",
      "208/208 [==============================] - 0s 533us/sample - loss: 0.4200 - accuracy: 0.8413\n",
      "Epoch 19/100\n",
      "208/208 [==============================] - 0s 498us/sample - loss: 0.4085 - accuracy: 0.8173\n",
      "Epoch 20/100\n",
      "208/208 [==============================] - 0s 490us/sample - loss: 0.4050 - accuracy: 0.8221\n",
      "Epoch 21/100\n",
      "208/208 [==============================] - 0s 514us/sample - loss: 0.3964 - accuracy: 0.8317\n",
      "Epoch 22/100\n",
      "208/208 [==============================] - 0s 529us/sample - loss: 0.3940 - accuracy: 0.8173\n",
      "Epoch 23/100\n",
      "208/208 [==============================] - 0s 500us/sample - loss: 0.3824 - accuracy: 0.8365\n",
      "Epoch 24/100\n",
      "208/208 [==============================] - 0s 510us/sample - loss: 0.3855 - accuracy: 0.8221\n",
      "Epoch 25/100\n",
      "208/208 [==============================] - 0s 503us/sample - loss: 0.3743 - accuracy: 0.8510\n",
      "Epoch 26/100\n",
      "208/208 [==============================] - 0s 486us/sample - loss: 0.3696 - accuracy: 0.8462\n",
      "Epoch 27/100\n",
      "208/208 [==============================] - 0s 515us/sample - loss: 0.3651 - accuracy: 0.8269\n",
      "Epoch 28/100\n",
      "208/208 [==============================] - 0s 561us/sample - loss: 0.3551 - accuracy: 0.8365\n",
      "Epoch 29/100\n",
      "208/208 [==============================] - 0s 538us/sample - loss: 0.3613 - accuracy: 0.8365\n",
      "Epoch 30/100\n",
      "208/208 [==============================] - 0s 577us/sample - loss: 0.3446 - accuracy: 0.8558\n",
      "Epoch 31/100\n",
      "208/208 [==============================] - 0s 577us/sample - loss: 0.3423 - accuracy: 0.8606\n",
      "Epoch 32/100\n",
      "208/208 [==============================] - 0s 578us/sample - loss: 0.3493 - accuracy: 0.8221\n",
      "Epoch 33/100\n",
      "208/208 [==============================] - 0s 508us/sample - loss: 0.3393 - accuracy: 0.8413\n",
      "Epoch 34/100\n",
      "208/208 [==============================] - 0s 555us/sample - loss: 0.3292 - accuracy: 0.8702\n",
      "Epoch 35/100\n",
      "208/208 [==============================] - 0s 592us/sample - loss: 0.3274 - accuracy: 0.8654\n",
      "Epoch 36/100\n",
      "208/208 [==============================] - 0s 521us/sample - loss: 0.3198 - accuracy: 0.8654\n",
      "Epoch 37/100\n",
      "208/208 [==============================] - 0s 593us/sample - loss: 0.3287 - accuracy: 0.8846\n",
      "Epoch 38/100\n",
      "208/208 [==============================] - 0s 572us/sample - loss: 0.3154 - accuracy: 0.8750\n",
      "Epoch 39/100\n",
      "208/208 [==============================] - 0s 565us/sample - loss: 0.3311 - accuracy: 0.8606\n",
      "Epoch 40/100\n",
      "208/208 [==============================] - 0s 543us/sample - loss: 0.3039 - accuracy: 0.8798\n",
      "Epoch 41/100\n",
      "208/208 [==============================] - 0s 591us/sample - loss: 0.3075 - accuracy: 0.8798\n",
      "Epoch 42/100\n",
      "208/208 [==============================] - 0s 570us/sample - loss: 0.2992 - accuracy: 0.8654\n",
      "Epoch 43/100\n",
      "208/208 [==============================] - 0s 580us/sample - loss: 0.2965 - accuracy: 0.8798\n",
      "Epoch 44/100\n",
      "208/208 [==============================] - 0s 520us/sample - loss: 0.2984 - accuracy: 0.8942\n",
      "Epoch 45/100\n",
      "208/208 [==============================] - 0s 563us/sample - loss: 0.2874 - accuracy: 0.9038\n",
      "Epoch 46/100\n",
      "208/208 [==============================] - 0s 545us/sample - loss: 0.2821 - accuracy: 0.8846\n",
      "Epoch 47/100\n",
      "208/208 [==============================] - 0s 574us/sample - loss: 0.2869 - accuracy: 0.8942\n",
      "Epoch 48/100\n",
      "208/208 [==============================] - 0s 615us/sample - loss: 0.2776 - accuracy: 0.8846\n",
      "Epoch 49/100\n",
      "208/208 [==============================] - 0s 598us/sample - loss: 0.2768 - accuracy: 0.8894\n",
      "Epoch 50/100\n",
      "208/208 [==============================] - 0s 601us/sample - loss: 0.2735 - accuracy: 0.8846\n",
      "Epoch 51/100\n",
      "208/208 [==============================] - 0s 577us/sample - loss: 0.2674 - accuracy: 0.8846\n",
      "Epoch 52/100\n",
      "208/208 [==============================] - 0s 577us/sample - loss: 0.2780 - accuracy: 0.8846\n",
      "Epoch 53/100\n",
      "208/208 [==============================] - 0s 615us/sample - loss: 0.2643 - accuracy: 0.9038\n",
      "Epoch 54/100\n",
      "208/208 [==============================] - 0s 615us/sample - loss: 0.2590 - accuracy: 0.9087\n",
      "Epoch 55/100\n",
      "208/208 [==============================] - 0s 596us/sample - loss: 0.2552 - accuracy: 0.8846\n",
      "Epoch 56/100\n",
      "208/208 [==============================] - 0s 538us/sample - loss: 0.2510 - accuracy: 0.9231\n",
      "Epoch 57/100\n",
      "208/208 [==============================] - 0s 493us/sample - loss: 0.2547 - accuracy: 0.9087\n",
      "Epoch 58/100\n",
      "208/208 [==============================] - 0s 577us/sample - loss: 0.2435 - accuracy: 0.9279\n",
      "Epoch 59/100\n",
      "208/208 [==============================] - 0s 607us/sample - loss: 0.2417 - accuracy: 0.9327\n",
      "Epoch 60/100\n",
      "208/208 [==============================] - 0s 865us/sample - loss: 0.2445 - accuracy: 0.9135\n",
      "Epoch 61/100\n",
      "208/208 [==============================] - 0s 539us/sample - loss: 0.2328 - accuracy: 0.9375\n",
      "Epoch 62/100\n",
      "208/208 [==============================] - 0s 558us/sample - loss: 0.2379 - accuracy: 0.9231\n",
      "Epoch 63/100\n",
      "208/208 [==============================] - 0s 596us/sample - loss: 0.2329 - accuracy: 0.9183\n",
      "Epoch 64/100\n",
      "208/208 [==============================] - 0s 577us/sample - loss: 0.2266 - accuracy: 0.9135\n",
      "Epoch 65/100\n",
      "208/208 [==============================] - 0s 596us/sample - loss: 0.2230 - accuracy: 0.9183\n",
      "Epoch 66/100\n",
      "208/208 [==============================] - 0s 578us/sample - loss: 0.2254 - accuracy: 0.9231\n",
      "Epoch 67/100\n",
      "208/208 [==============================] - 0s 519us/sample - loss: 0.2381 - accuracy: 0.9135\n",
      "Epoch 68/100\n",
      "208/208 [==============================] - 0s 500us/sample - loss: 0.2169 - accuracy: 0.9231\n",
      "Epoch 69/100\n",
      "208/208 [==============================] - 0s 481us/sample - loss: 0.2185 - accuracy: 0.9231\n",
      "Epoch 70/100\n",
      "208/208 [==============================] - 0s 489us/sample - loss: 0.2045 - accuracy: 0.9375\n",
      "Epoch 71/100\n",
      "208/208 [==============================] - 0s 503us/sample - loss: 0.2117 - accuracy: 0.9183\n",
      "Epoch 72/100\n",
      "208/208 [==============================] - 0s 486us/sample - loss: 0.2020 - accuracy: 0.9519\n",
      "Epoch 73/100\n",
      "208/208 [==============================] - 0s 519us/sample - loss: 0.2014 - accuracy: 0.9567\n",
      "Epoch 74/100\n",
      "208/208 [==============================] - 0s 523us/sample - loss: 0.2019 - accuracy: 0.9327\n",
      "Epoch 75/100\n",
      "208/208 [==============================] - 0s 504us/sample - loss: 0.1940 - accuracy: 0.9471\n",
      "Epoch 76/100\n",
      "208/208 [==============================] - 0s 499us/sample - loss: 0.2006 - accuracy: 0.9327\n",
      "Epoch 77/100\n",
      "208/208 [==============================] - 0s 513us/sample - loss: 0.1888 - accuracy: 0.9327\n",
      "Epoch 78/100\n",
      "208/208 [==============================] - 0s 486us/sample - loss: 0.1959 - accuracy: 0.9327\n",
      "Epoch 79/100\n",
      "208/208 [==============================] - 0s 465us/sample - loss: 0.1863 - accuracy: 0.9615\n",
      "Epoch 80/100\n",
      "208/208 [==============================] - 0s 457us/sample - loss: 0.1856 - accuracy: 0.9663\n",
      "Epoch 81/100\n",
      "208/208 [==============================] - 0s 481us/sample - loss: 0.1779 - accuracy: 0.9567\n",
      "Epoch 82/100\n",
      "208/208 [==============================] - 0s 465us/sample - loss: 0.1789 - accuracy: 0.9615\n",
      "Epoch 83/100\n",
      "208/208 [==============================] - 0s 487us/sample - loss: 0.1752 - accuracy: 0.9519\n",
      "Epoch 84/100\n",
      "208/208 [==============================] - 0s 469us/sample - loss: 0.1733 - accuracy: 0.9567\n",
      "Epoch 85/100\n",
      "208/208 [==============================] - 0s 488us/sample - loss: 0.1714 - accuracy: 0.9567\n",
      "Epoch 86/100\n",
      "208/208 [==============================] - 0s 503us/sample - loss: 0.1766 - accuracy: 0.9423\n",
      "Epoch 87/100\n",
      "208/208 [==============================] - 0s 521us/sample - loss: 0.1711 - accuracy: 0.9471\n",
      "Epoch 88/100\n",
      "208/208 [==============================] - 0s 558us/sample - loss: 0.1664 - accuracy: 0.9567\n",
      "Epoch 89/100\n",
      "208/208 [==============================] - 0s 498us/sample - loss: 0.1618 - accuracy: 0.9567\n",
      "Epoch 90/100\n",
      "208/208 [==============================] - 0s 495us/sample - loss: 0.1618 - accuracy: 0.9567\n",
      "Epoch 91/100\n",
      "208/208 [==============================] - 0s 515us/sample - loss: 0.1668 - accuracy: 0.9471\n",
      "Epoch 92/100\n",
      "208/208 [==============================] - 0s 508us/sample - loss: 0.1550 - accuracy: 0.9663\n",
      "Epoch 93/100\n",
      "208/208 [==============================] - 0s 516us/sample - loss: 0.1581 - accuracy: 0.9567\n",
      "Epoch 94/100\n",
      "208/208 [==============================] - 0s 487us/sample - loss: 0.1528 - accuracy: 0.9663\n",
      "Epoch 95/100\n",
      "208/208 [==============================] - 0s 477us/sample - loss: 0.1524 - accuracy: 0.9615\n",
      "Epoch 96/100\n",
      "208/208 [==============================] - 0s 518us/sample - loss: 0.1451 - accuracy: 0.9760\n",
      "Epoch 97/100\n",
      "208/208 [==============================] - 0s 491us/sample - loss: 0.1459 - accuracy: 0.9712\n",
      "Epoch 98/100\n",
      "208/208 [==============================] - 0s 549us/sample - loss: 0.1478 - accuracy: 0.9712\n",
      "Epoch 99/100\n",
      "208/208 [==============================] - 0s 591us/sample - loss: 0.1390 - accuracy: 0.9712\n",
      "Epoch 100/100\n",
      "208/208 [==============================] - 0s 603us/sample - loss: 0.1472 - accuracy: 0.9663\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2811a576048>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='Adam' , loss='binary_crossentropy' , metrics=[('accuracy')])\n",
    "model.fit(x , A , epochs =100 , batch_size =5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP #8\n",
    "Rewriting the code by doing Model Subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tensorflow.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1=layers.Dense(30 , activation='relu')\n",
    "        self.dense2=layers.Dense(1 , activation='sigmoid')\n",
    "        \n",
    "    def call(self , inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        return self.dense2(x)\n",
    "    \n",
    "model=MyModel() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 208 samples\n",
      "Epoch 1/100\n",
      "208/208 [==============================] - 1s 6ms/sample - loss: 0.6705 - accuracy: 0.5721\n",
      "Epoch 2/100\n",
      "208/208 [==============================] - 0s 1ms/sample - loss: 0.6444 - accuracy: 0.6058\n",
      "Epoch 3/100\n",
      "208/208 [==============================] - 0s 905us/sample - loss: 0.6171 - accuracy: 0.7308\n",
      "Epoch 4/100\n",
      "208/208 [==============================] - 0s 664us/sample - loss: 0.5997 - accuracy: 0.7308\n",
      "Epoch 5/100\n",
      "208/208 [==============================] - 0s 646us/sample - loss: 0.5768 - accuracy: 0.7596\n",
      "Epoch 6/100\n",
      "208/208 [==============================] - 0s 376us/sample - loss: 0.5613 - accuracy: 0.7644\n",
      "Epoch 7/100\n",
      "208/208 [==============================] - 0s 388us/sample - loss: 0.5428 - accuracy: 0.7885\n",
      "Epoch 8/100\n",
      "208/208 [==============================] - 0s 404us/sample - loss: 0.5253 - accuracy: 0.7933\n",
      "Epoch 9/100\n",
      "208/208 [==============================] - 0s 405us/sample - loss: 0.5137 - accuracy: 0.7933\n",
      "Epoch 10/100\n",
      "208/208 [==============================] - 0s 406us/sample - loss: 0.5001 - accuracy: 0.8029\n",
      "Epoch 11/100\n",
      "208/208 [==============================] - 0s 401us/sample - loss: 0.4915 - accuracy: 0.7596\n",
      "Epoch 12/100\n",
      "208/208 [==============================] - 0s 402us/sample - loss: 0.4824 - accuracy: 0.7885\n",
      "Epoch 13/100\n",
      "208/208 [==============================] - 0s 413us/sample - loss: 0.4675 - accuracy: 0.8173\n",
      "Epoch 14/100\n",
      "208/208 [==============================] - 0s 419us/sample - loss: 0.4575 - accuracy: 0.8173\n",
      "Epoch 15/100\n",
      "208/208 [==============================] - 0s 431us/sample - loss: 0.4535 - accuracy: 0.8269\n",
      "Epoch 16/100\n",
      "208/208 [==============================] - 0s 399us/sample - loss: 0.4555 - accuracy: 0.8125\n",
      "Epoch 17/100\n",
      "208/208 [==============================] - 0s 395us/sample - loss: 0.4352 - accuracy: 0.8413\n",
      "Epoch 18/100\n",
      "208/208 [==============================] - 0s 416us/sample - loss: 0.4317 - accuracy: 0.8221\n",
      "Epoch 19/100\n",
      "208/208 [==============================] - 0s 411us/sample - loss: 0.4312 - accuracy: 0.8077\n",
      "Epoch 20/100\n",
      "208/208 [==============================] - 0s 402us/sample - loss: 0.4176 - accuracy: 0.8413\n",
      "Epoch 21/100\n",
      "208/208 [==============================] - 0s 400us/sample - loss: 0.4124 - accuracy: 0.8317\n",
      "Epoch 22/100\n",
      "208/208 [==============================] - 0s 408us/sample - loss: 0.4039 - accuracy: 0.8558\n",
      "Epoch 23/100\n",
      "208/208 [==============================] - 0s 413us/sample - loss: 0.3967 - accuracy: 0.8462\n",
      "Epoch 24/100\n",
      "208/208 [==============================] - 0s 420us/sample - loss: 0.3935 - accuracy: 0.8462\n",
      "Epoch 25/100\n",
      "208/208 [==============================] - 0s 414us/sample - loss: 0.3885 - accuracy: 0.8462\n",
      "Epoch 26/100\n",
      "208/208 [==============================] - 0s 418us/sample - loss: 0.3875 - accuracy: 0.8221\n",
      "Epoch 27/100\n",
      "208/208 [==============================] - 0s 404us/sample - loss: 0.3786 - accuracy: 0.8510\n",
      "Epoch 28/100\n",
      "208/208 [==============================] - 0s 419us/sample - loss: 0.3729 - accuracy: 0.8317\n",
      "Epoch 29/100\n",
      "208/208 [==============================] - 0s 422us/sample - loss: 0.3749 - accuracy: 0.8510\n",
      "Epoch 30/100\n",
      "208/208 [==============================] - 0s 455us/sample - loss: 0.3723 - accuracy: 0.8558\n",
      "Epoch 31/100\n",
      "208/208 [==============================] - 0s 400us/sample - loss: 0.3645 - accuracy: 0.8606\n",
      "Epoch 32/100\n",
      "208/208 [==============================] - 0s 402us/sample - loss: 0.3741 - accuracy: 0.8413\n",
      "Epoch 33/100\n",
      "208/208 [==============================] - 0s 403us/sample - loss: 0.3553 - accuracy: 0.8510\n",
      "Epoch 34/100\n",
      "208/208 [==============================] - 0s 410us/sample - loss: 0.3604 - accuracy: 0.8654\n",
      "Epoch 35/100\n",
      "208/208 [==============================] - 0s 400us/sample - loss: 0.3503 - accuracy: 0.8654\n",
      "Epoch 36/100\n",
      "208/208 [==============================] - 0s 419us/sample - loss: 0.3486 - accuracy: 0.8462\n",
      "Epoch 37/100\n",
      "208/208 [==============================] - 0s 400us/sample - loss: 0.3382 - accuracy: 0.8702\n",
      "Epoch 38/100\n",
      "208/208 [==============================] - 0s 425us/sample - loss: 0.3405 - accuracy: 0.8606\n",
      "Epoch 39/100\n",
      "208/208 [==============================] - 0s 430us/sample - loss: 0.3331 - accuracy: 0.8654\n",
      "Epoch 40/100\n",
      "208/208 [==============================] - 0s 406us/sample - loss: 0.3327 - accuracy: 0.8750\n",
      "Epoch 41/100\n",
      "208/208 [==============================] - 0s 414us/sample - loss: 0.3310 - accuracy: 0.8942\n",
      "Epoch 42/100\n",
      "208/208 [==============================] - 0s 410us/sample - loss: 0.3234 - accuracy: 0.8846\n",
      "Epoch 43/100\n",
      "208/208 [==============================] - 0s 424us/sample - loss: 0.3210 - accuracy: 0.8654\n",
      "Epoch 44/100\n",
      "208/208 [==============================] - 0s 403us/sample - loss: 0.3192 - accuracy: 0.8606\n",
      "Epoch 45/100\n",
      "208/208 [==============================] - 0s 407us/sample - loss: 0.3097 - accuracy: 0.8846\n",
      "Epoch 46/100\n",
      "208/208 [==============================] - 0s 403us/sample - loss: 0.3105 - accuracy: 0.8894\n",
      "Epoch 47/100\n",
      "208/208 [==============================] - 0s 406us/sample - loss: 0.3061 - accuracy: 0.8942\n",
      "Epoch 48/100\n",
      "208/208 [==============================] - 0s 411us/sample - loss: 0.3057 - accuracy: 0.8846\n",
      "Epoch 49/100\n",
      "208/208 [==============================] - 0s 414us/sample - loss: 0.2999 - accuracy: 0.8990\n",
      "Epoch 50/100\n",
      "208/208 [==============================] - 0s 394us/sample - loss: 0.2999 - accuracy: 0.8942\n",
      "Epoch 51/100\n",
      "208/208 [==============================] - 0s 420us/sample - loss: 0.2942 - accuracy: 0.8990\n",
      "Epoch 52/100\n",
      "208/208 [==============================] - 0s 418us/sample - loss: 0.2936 - accuracy: 0.8894\n",
      "Epoch 53/100\n",
      "208/208 [==============================] - 0s 413us/sample - loss: 0.2856 - accuracy: 0.8894\n",
      "Epoch 54/100\n",
      "208/208 [==============================] - 0s 422us/sample - loss: 0.3000 - accuracy: 0.8798\n",
      "Epoch 55/100\n",
      "208/208 [==============================] - 0s 402us/sample - loss: 0.2833 - accuracy: 0.9135\n",
      "Epoch 56/100\n",
      "208/208 [==============================] - 0s 437us/sample - loss: 0.2811 - accuracy: 0.9087\n",
      "Epoch 57/100\n",
      "208/208 [==============================] - 0s 442us/sample - loss: 0.2817 - accuracy: 0.8990\n",
      "Epoch 58/100\n",
      "208/208 [==============================] - 0s 461us/sample - loss: 0.2793 - accuracy: 0.8942\n",
      "Epoch 59/100\n",
      "208/208 [==============================] - 0s 424us/sample - loss: 0.2753 - accuracy: 0.8894\n",
      "Epoch 60/100\n",
      "208/208 [==============================] - 0s 419us/sample - loss: 0.3016 - accuracy: 0.8606\n",
      "Epoch 61/100\n",
      "208/208 [==============================] - 0s 405us/sample - loss: 0.2755 - accuracy: 0.8990\n",
      "Epoch 62/100\n",
      "208/208 [==============================] - 0s 396us/sample - loss: 0.2644 - accuracy: 0.9183\n",
      "Epoch 63/100\n",
      "208/208 [==============================] - 0s 399us/sample - loss: 0.2649 - accuracy: 0.9231\n",
      "Epoch 64/100\n",
      "208/208 [==============================] - 0s 397us/sample - loss: 0.2659 - accuracy: 0.9087\n",
      "Epoch 65/100\n",
      "208/208 [==============================] - 0s 416us/sample - loss: 0.2590 - accuracy: 0.9087\n",
      "Epoch 66/100\n",
      "208/208 [==============================] - 0s 399us/sample - loss: 0.2544 - accuracy: 0.9183\n",
      "Epoch 67/100\n",
      "208/208 [==============================] - 0s 422us/sample - loss: 0.2558 - accuracy: 0.9183\n",
      "Epoch 68/100\n",
      "208/208 [==============================] - 0s 415us/sample - loss: 0.2523 - accuracy: 0.9327\n",
      "Epoch 69/100\n",
      "208/208 [==============================] - 0s 402us/sample - loss: 0.2488 - accuracy: 0.9231\n",
      "Epoch 70/100\n",
      "208/208 [==============================] - 0s 449us/sample - loss: 0.2495 - accuracy: 0.9135\n",
      "Epoch 71/100\n",
      "208/208 [==============================] - 0s 433us/sample - loss: 0.2448 - accuracy: 0.9231\n",
      "Epoch 72/100\n",
      "208/208 [==============================] - 0s 424us/sample - loss: 0.2472 - accuracy: 0.9231\n",
      "Epoch 73/100\n",
      "208/208 [==============================] - 0s 436us/sample - loss: 0.2381 - accuracy: 0.9279\n",
      "Epoch 74/100\n",
      "208/208 [==============================] - 0s 473us/sample - loss: 0.2333 - accuracy: 0.9279\n",
      "Epoch 75/100\n",
      "208/208 [==============================] - 0s 457us/sample - loss: 0.2382 - accuracy: 0.9279\n",
      "Epoch 76/100\n",
      "208/208 [==============================] - 0s 452us/sample - loss: 0.2331 - accuracy: 0.9327\n",
      "Epoch 77/100\n",
      "208/208 [==============================] - 0s 435us/sample - loss: 0.2382 - accuracy: 0.9327\n",
      "Epoch 78/100\n",
      "208/208 [==============================] - 0s 474us/sample - loss: 0.2300 - accuracy: 0.9279\n",
      "Epoch 79/100\n",
      "208/208 [==============================] - 0s 415us/sample - loss: 0.2261 - accuracy: 0.9375\n",
      "Epoch 80/100\n",
      "208/208 [==============================] - 0s 409us/sample - loss: 0.2224 - accuracy: 0.9327\n",
      "Epoch 81/100\n",
      "208/208 [==============================] - 0s 456us/sample - loss: 0.2222 - accuracy: 0.9327\n",
      "Epoch 82/100\n",
      "208/208 [==============================] - 0s 402us/sample - loss: 0.2249 - accuracy: 0.9183\n",
      "Epoch 83/100\n",
      "208/208 [==============================] - 0s 418us/sample - loss: 0.2185 - accuracy: 0.9423\n",
      "Epoch 84/100\n",
      "208/208 [==============================] - 0s 388us/sample - loss: 0.2126 - accuracy: 0.9375\n",
      "Epoch 85/100\n",
      "208/208 [==============================] - 0s 400us/sample - loss: 0.2127 - accuracy: 0.9327\n",
      "Epoch 86/100\n",
      "208/208 [==============================] - 0s 397us/sample - loss: 0.2153 - accuracy: 0.9327\n",
      "Epoch 87/100\n",
      "208/208 [==============================] - 0s 402us/sample - loss: 0.2061 - accuracy: 0.9327\n",
      "Epoch 88/100\n",
      "208/208 [==============================] - 0s 404us/sample - loss: 0.2131 - accuracy: 0.9471\n",
      "Epoch 89/100\n",
      "208/208 [==============================] - 0s 417us/sample - loss: 0.2105 - accuracy: 0.9375\n",
      "Epoch 90/100\n",
      "208/208 [==============================] - 0s 410us/sample - loss: 0.2031 - accuracy: 0.9327\n",
      "Epoch 91/100\n",
      "208/208 [==============================] - 0s 391us/sample - loss: 0.2193 - accuracy: 0.9279\n",
      "Epoch 92/100\n",
      "208/208 [==============================] - 0s 394us/sample - loss: 0.2010 - accuracy: 0.9471\n",
      "Epoch 93/100\n",
      "208/208 [==============================] - 0s 445us/sample - loss: 0.2030 - accuracy: 0.9375\n",
      "Epoch 94/100\n",
      "208/208 [==============================] - 0s 416us/sample - loss: 0.2036 - accuracy: 0.9423\n",
      "Epoch 95/100\n",
      "208/208 [==============================] - 0s 422us/sample - loss: 0.1993 - accuracy: 0.9567\n",
      "Epoch 96/100\n",
      "208/208 [==============================] - 0s 395us/sample - loss: 0.1918 - accuracy: 0.9519\n",
      "Epoch 97/100\n",
      "208/208 [==============================] - 0s 403us/sample - loss: 0.1903 - accuracy: 0.9375\n",
      "Epoch 98/100\n",
      "208/208 [==============================] - 0s 404us/sample - loss: 0.1843 - accuracy: 0.9615\n",
      "Epoch 99/100\n",
      "208/208 [==============================] - 0s 418us/sample - loss: 0.1986 - accuracy: 0.9519\n",
      "Epoch 100/100\n",
      "208/208 [==============================] - 0s 403us/sample - loss: 0.1864 - accuracy: 0.9519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e2efabb48>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='Adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "model.fit(X , A, epochs = 100, batch_size = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
